# ðŸ”¬ Critical Research Concerns and Recommendations for Fair-CB

**Reviewed by:** AI Research Analysis  
**Date:** 2026-01-31 16:52:13  
**Repository:** DevDaring/Bandit_Debiasing  

---

## Executive Summary

After deep analysis of the Fair-CB framework, I have identified **17 critical concerns** across theoretical foundations, experimental design, metric validity, and result registration. While the research direction is promising, several issues could undermine the validity of published results.

**Severity Classification:**
- ðŸ”´ **CRITICAL** - Results may be invalid or misleading
- ðŸŸ  **MAJOR** - Significant scientific concerns
- ðŸŸ¡ **MODERATE** - Should be addressed before publication
- ðŸŸ¢ **MINOR** - Improvements for robustness

---

## 1. THEORETICAL CONCERNS

### ðŸ”´ 1.1 CRITICAL: Oracle Regret Computation is Fundamentally Flawed

**Location:** `src/theory/regret_tracker.py` (lines 80-108)

**Problem:** The regret computation requires `optimal_reward` to calculate instantaneous regret:
```python
if optimal_reward is not None:
    inst_regret = optimal_reward - reward
elif self.compute_optimal and len(self.arm_rewards[selected_arm]) > 10:
    # Estimate optimal from observed arm means
```

**Loophole:** 
1. You **cannot know the optimal reward** without actually running ALL arms on EVERY sample
2. The fallback "estimate optimal from observed arm means" is **biased** - it only uses observed rewards for selected arms, not counterfactual rewards
3. This creates a **selection bias**: frequently selected arms will have more accurate mean estimates

**Impact:** Your reported regret bounds may be **significantly underestimated** because:
- You're comparing against estimated (not true) optimal
- The estimation improves as the algorithm converges, artificially showing "decreasing regret"

**Recommendation:**
```python
# REQUIRED: Run ALL arms on a subset of samples to get true counterfactual rewards
# This is expensive but scientifically necessary
def compute_true_regret(self, context, selected_arm, all_arm_rewards):
    """
    Requires running all 6 arms on the same input.
    all_arm_rewards: Dict[int, float] with actual rewards from all arms
    """
    optimal_reward = max(all_arm_rewards.values())
    return optimal_reward - all_arm_rewards[selected_arm]
```

---

### ðŸ”´ 1.2 CRITICAL: Adaptive vs Static Comparison (Theorem 3) is Not Provable

**Location:** `src/theory/adaptive_vs_static.py`

**Problem:** The claim that `R_adaptive/R_static â†’ 0` cannot be verified without counterfactual data.

**In your code:**
```python
def update(self, selected_arm, reward, counterfactual_rewards=None):
    # counterfactual_rewards is almost always None!
```

**Reality:** Without running all arms on every sample, you cannot compute:
- What the "best static arm" would have achieved
- The true adaptive regret

**Loophole:** Your "best static arm" is computed from **partial observations** of selected arms only. This is **not** the same as the true best static strategy.

---

### ðŸŸ  1.3 MAJOR: LinUCB Bound Assumptions May Not Hold

**Location:** `src/theory/bounds.py` (lines 30-60)

**Problem:** The LinUCB bound `R(T) â‰¤ O(dâˆš(KT log(T/Î´)))` requires:

1. **Realizability**: True reward function is linear in context
2. **Bounded context**: `||x_t|| â‰¤ L` for all t
3. **Sub-Gaussian noise**: Reward noise is sub-Gaussian
4. **i.i.d. contexts**: Contexts are drawn i.i.d.

**Your violations:**
1. Bias scores are NOT linear functions of 128-dim context
2. You normalize context vectors (good), but `L` is never verified
3. Bias scoring involves neural embedding models - noise is NOT sub-Gaussian
4. Contexts from Multi-CrowS-Pairs are NOT i.i.d. (grouped by bias category)

**Impact:** Theoretical bounds may not actually hold for your setup.

---

### ðŸŸ  1.4 MAJOR: Monte Carlo Verification is Circular

**Location:** `src/theory/theorem_verification.py` (lines 80-108)

**Problem:** The verification simulates a **fake** bandit environment:
```python
# True arm means (unknown to bandit)
arm_means = np.random.uniform(0.3, 0.8, size=self.n_arms)
```

This does NOT verify that bounds hold on your **actual** LLM debiasing task. It only verifies LinUCB works on synthetic linear bandits (which is already known).

---

## 2. EXPERIMENTAL DESIGN CONCERNS

### ðŸ”´ 2.1 CRITICAL: Reward Signal Contamination

**Location:** `src/reward/bias_scorer.py` and `src/reward/reward_calculator.py`

**Problem:** The same embedding model (`paraphrase-multilingual-MiniLM-L12-v2`) is used for:
1. **Bias scoring** (in `bias_scorer.py`)
2. **Quality scoring** (in `quality_scorer.py`)  
3. **Context encoding** (in `context_encoder.py`)

**Loophole:** This creates **information leakage**:
- The bandit learns to exploit quirks of the embedding model, not actual bias
- Improvements in "bias score" may reflect overfitting to embedding space

**Recommendation:** Use DIFFERENT models for context vs reward computation.

---

### ðŸ”´ 2.2 CRITICAL: Steering Vector Training Data Overlap

**Location:** `My_Improvement_Prompts.md` and `src/debiasing_arms/steering_vector_arm.py`

**Critical Question:** Are steering vectors trained on the SAME datasets (Multi-CrowS-Pairs, IndiBias) that you use for evaluation?

If YES â†’ **Results are invalid** due to train-test contamination.

**Recommendation:** Explicitly document:
1. What data was used to create steering vectors
2. Ensure ZERO overlap with evaluation data
3. Use separate splits or entirely different datasets

---

### ðŸŸ  2.3 MAJOR: Evaluation Metric Computes Biased Estimates

**Location:** `src/metrics/ibr.py` (lines 60-80)

**Problem:** IBR computes bias reduction as:
```python
reduction = (baseline_bias - method_bias) / baseline_bias
```

**Issues:**
1. If `baseline_bias â‰ˆ 0` (low bias input), you get division instability
2. Negative reduction (method increases bias) is clipped: `np.clip(reduction, -1.0, 1.0)`
3. Harmonic mean ignores categories with increased bias (filtered out)

**Loophole:** A method that **increases** bias in some categories but reduces in others can still get high IBR because negative reductions are excluded.

**Your code:**
```python
positive_reductions = {
    cat: max(br, epsilon)
    for cat, br in bias_reductions.items()
    if br > 0  # <-- IGNORES NEGATIVE REDUCTIONS!
}
```

---

### ðŸŸ  2.4 MAJOR: Model Mismatch Between README and Prompts

**README.md states:**
- Qwen 2.5 **7B**, Aya Expanse **8B**, Llama 3.1 **8B**

**My_Improvement_Prompts.md states:**
- Qwen2.5-**1.5B**-Instruct, Llama-3.2-**1B**-Instruct, Gemma-2-**2B**-it

**Impact:** Readers may be confused about which models were actually used. This affects reproducibility.

---

### ðŸŸ¡ 2.5 MODERATE: Warmup Strategy May Bias Arm Selection

**Location:** `src/pipeline/sequential_training_pipeline.py`

**Problem:** During warmup, you likely use uniform random arm selection. However:
- Some arms (steering vectors) have higher computational cost
- If warmup is too short, bandit may not explore all arms sufficiently
- If warmup is too long, you waste samples

**Recommendation:** Report arm selection frequency during AND after warmup to ensure fair exploration.

---

## 3. RESULT REGISTRATION CONCERNS

### ðŸ”´ 3.1 CRITICAL: Results Will NOT Be Properly Registered Without These Changes

**Problem 1: Counterfactual Rewards**
- Your regret tracking requires `optimal_reward` but this is rarely available
- Results will show **estimated** regret, not **true** regret

**Problem 2: Per-Sample Arm Performance**
- To claim "adaptive > static", you need to run ALL arms on evaluation samples
- Currently, you only run the SELECTED arm

**Problem 3: Statistical Significance**
- I don't see confidence intervals or significance tests in your evaluation code
- Without p-values, you cannot claim improvements are significant

**REQUIRED for valid results:**
```python
# For EACH evaluation sample:
for sample in eval_data:
    results_per_arm = {}
    for arm in range(6):
        output = generate_with_arm(model, sample, arm)
        results_per_arm[arm] = {
            'bias_score': compute_bias(output),
            'quality_score': compute_quality(output),
            'reward': compute_reward(output)
        }
    
    selected_arm = bandit.select_arm(context)
    optimal_arm = argmax(results_per_arm, key='reward')
    
    # NOW you have true counterfactual data for regret computation
```

---

### ðŸŸ  3.2 MAJOR: CSV Output Missing Critical Columns

**Location:** `My_Improvement_Prompts.md` specifies CSV format

**Missing columns for valid research:**
1. `Confidence_Interval_95` - for all metrics
2. `Statistical_Significance_P_Value` - for comparisons
3. `Number_Of_Samples` - per configuration
4. `Random_Seed` - for reproducibility
5. `Counterfactual_Rewards_Computed` - boolean flag

---

## 4. CODE-LEVEL ISSUES

### ðŸŸ¡ 4.1 MODERATE: Steering Vector Hook Cleanup

**Location:** `src/debiasing_arms/steering_vector_arm.py`

**Problem:** Hooks are registered but I don't see explicit cleanup:
```python
self.active_hooks = []
# Where is hook.remove() called?
```

**Impact:** Memory leaks and potential interference between arms.

---

### ðŸŸ¡ 4.2 MODERATE: Topic Classifier Uses Large External Model

**Location:** `src/context_extractor/topic_classifier.py`

```python
model="facebook/bart-large-mnli"
```

**Problem:** This is a 407M parameter model loaded alongside your LLM. With 24GB VRAM constraint, this may cause issues.

---

### ðŸŸ¡ 4.3 MODERATE: Hardcoded Token Adjustments

**Location:** `src/debiasing_arms/output_adjustment_arm.py`

The token adjustments are English-only:
```python
'he': -2.0, 'she': -2.0, 'they': +1.0
```

**Problem:** This arm will NOT work for Hindi/Bengali inputs.

---

## 5. RECOMMENDED EXPERIMENTS TO ADD

### 5.1 Required Baseline Comparisons

| Experiment | Purpose |
|------------|---------|
| **Random Arm Selection** | Verify bandit learns better than random |
| **Best Static Arm (Oracle)** | Requires full counterfactual data |
| **Per-Category Best Static** | Different optimal arm per bias type? |
| **Prompt-Only Baseline** | Is steering necessary? |

### 5.2 Ablation Studies Missing

| Ablation | Question |
|----------|----------|
| Context dimension | Is 128-dim necessary? Try 32, 64, 256 |
| Steering strength | Vary strength from 0.1 to 2.0 |
| Fairness threshold | Test Ï„ âˆˆ {0.1, 0.2, 0.3, 0.4, 0.5} |
| Lambda weight | Test Î» âˆˆ {0.0, 0.25, 0.5, 0.75, 1.0} |

### 5.3 Cross-Validation Required

- **K-Fold CV**: Report mean Â± std across 5 folds
- **Separate test set**: Hold out 20% completely unseen data
- **Multiple random seeds**: Run each experiment 3-5 times

---

## 6. PUBLICATION READINESS CHECKLIST

### Before Submitting to TACL:

- [ ] **Counterfactual evaluation**: Run all 6 arms on all eval samples
- [ ] **True regret computation**: Use actual optimal rewards
- [ ] **Statistical tests**: Add paired t-tests or bootstrap CIs
- [ ] **Steering vector documentation**: Prove no train-test overlap
- [ ] **Model consistency**: Fix 7B/8B vs 1.5B/2B discrepancy
- [ ] **Reproducibility**: Fixed seeds, version numbers, hardware specs
- [ ] **Negative results**: Report where method FAILS (which categories, languages)
- [ ] **Error analysis**: Manual inspection of 50+ failure cases
- [ ] **Human evaluation**: At minimum, verify 100 samples manually

---

## 7. POSITIVE ASPECTS

Despite the concerns, the research has significant strengths:

1. âœ… **Novel metrics** (IBR, FAR) are interesting contributions
2. âœ… **Multilingual focus** (English, Hindi, Bengali) is underexplored
3. âœ… **Caste bias** inclusion is valuable for Indian context
4. âœ… **Code organization** is clean and modular
5. âœ… **Multiple bandit algorithms** (LinUCB, Thompson, Neural) good comparison

---

## 8. SUMMARY OF REQUIRED CHANGES

| Priority | Issue | Effort |
|----------|-------|--------|
| ðŸ”´ CRITICAL | Implement counterfactual evaluation | HIGH |
| ðŸ”´ CRITICAL | Fix regret computation | MEDIUM |
| ðŸ”´ CRITICAL | Verify steering vector train-test split | LOW |
| ðŸ”´ CRITICAL | Fix IBR negative reduction handling | LOW |
| ðŸŸ  MAJOR | Add statistical significance tests | MEDIUM |
| ðŸŸ  MAJOR | Clarify model sizes used | LOW |
| ðŸŸ  MAJOR | Document theoretical assumption violations | MEDIUM |
| ðŸŸ¡ MODERATE | Add hook cleanup | LOW |
| ðŸŸ¡ MODERATE | Multilingual output adjustment | MEDIUM |

---

## 9. CONCLUSION

**Current State:** Results will NOT be properly registered due to fundamental issues in regret computation and lack of counterfactual evaluation.

**Path Forward:** 
1. Implement full counterfactual evaluation (run all arms on all samples)
2. Compute true regret from counterfactual rewards
3. Add statistical significance testing
4. Fix IBR to handle negative reductions properly
5. Document all theoretical assumption limitations

**Estimated Additional Work:** 2-4 weeks of engineering + experiments

---

*This analysis is provided to strengthen the research. The core idea of using contextual bandits for debiasing is sound and worth pursuing - these concerns ensure the published results will be scientifically valid.*