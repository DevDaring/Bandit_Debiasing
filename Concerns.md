# ðŸ”¬ Critical Research Concerns and Recommendations for Fair-CB

**Reviewed by:** AI Research Analysis  
**Date:** 2026-01-31  
**Repository:** DevDaring/Bandit_Debiasing  

---

## Executive Summary

After deep analysis of the Fair-CB framework, I have identified **23 critical concerns** across theoretical foundations, experimental design, metric validity, and result registration. While the research direction is promising and well-structured, several fundamental issues must be addressed before publication.

**Severity Classification:**
- ðŸ”´ **CRITICAL** - Results may be invalid or misleading
- ðŸŸ  **MAJOR** - Significant scientific concerns
- ðŸŸ¡ **MODERATE** - Should be addressed before publication
- ðŸŸ¢ **MINOR** - Improvements for robustness

---

## 1. THEORETICAL CONCERNS

### ðŸ”´ 1.1 CRITICAL: Oracle Regret Computation is Fundamentally Flawed

**Location:** `src/theory/regret_tracker.py` (lines 80-108)

**Problem:** The regret computation requires `optimal_reward` to calculate instantaneous regret:
```python
if optimal_reward is not None:
    inst_regret = optimal_reward - reward
elif self.compute_optimal and len(self.arm_rewards[selected_arm]) > 10:
    # Estimate optimal from observed arm means
```

**Loophole:** 
1. You **cannot know the optimal reward** without actually running ALL arms on EVERY sample
2. The fallback "estimate optimal from observed arm means" is **biased** - it only uses observed rewards for selected arms, not counterfactual rewards
3. This creates a **selection bias**: frequently selected arms will have more accurate mean estimates

**Impact:** Your reported regret bounds may be **significantly underestimated** because:
- You're comparing against estimated (not true) optimal
- The estimation improves as the algorithm converges, artificially showing "decreasing regret"

**Recommendation:**
```python
# REQUIRED: Run ALL arms on a subset of samples to get true counterfactual rewards
# This is expensive but scientifically necessary
def compute_true_regret(self, context, selected_arm, all_arm_rewards):
    """
    Requires running all 6 arms on the same input.
    all_arm_rewards: Dict[int, float] with actual rewards from all arms
    """
    optimal_reward = max(all_arm_rewards.values())
    return optimal_reward - all_arm_rewards[selected_arm]
```

**Good News:** You have implemented `CounterfactualEvaluator` in `src/evaluation/counterfactual_evaluator.py` which addresses this. **ENSURE** this is used for ALL publication results, not just final evaluation.

---

### ðŸ”´ 1.2 CRITICAL: Adaptive vs Static Comparison (Theorem 3) is Not Provable

**Location:** `src/theory/adaptive_vs_static.py`

**Problem:** The claim that `R_adaptive/R_static â†’ 0` cannot be verified without counterfactual data.

**In your code:**
```python
def update(self, selected_arm, reward, counterfactual_rewards=None):
    # counterfactual_rewards is almost always None!
```

**Reality:** Without running all arms on every sample, you cannot compute:
- What the "best static arm" would have achieved
- The true adaptive regret

**Loophole:** Your "best static arm" is computed from **partial observations** of selected arms only. This is **not** the same as the true best static strategy.

---

### ðŸŸ  1.3 MAJOR: LinUCB Bound Assumptions May Not Hold

**Location:** `src/theory/bounds.py` (lines 30-60)

**Problem:** The LinUCB bound `R(T) â‰¤ O(dâˆš(KT log(T/Î´)))` requires:

1. **Realizability**: True reward function is linear in context
2. **Bounded context**: `||x_t|| â‰¤ L` for all t
3. **Sub-Gaussian noise**: Reward noise is sub-Gaussian
4. **i.i.d. contexts**: Contexts are drawn i.i.d.

**Your violations:**
1. Bias scores are NOT linear functions of 128-dim context
2. You normalize context vectors (good), but `L` is never verified
3. Bias scoring involves neural embedding models - noise is NOT sub-Gaussian
4. Contexts from Multi-CrowS-Pairs are NOT i.i.d. (grouped by bias category)

**Impact:** Theoretical bounds may not actually hold for your setup.

**Mitigation documented in:** `docs/theoretical_limitations.md` - Good! But add quantitative analysis of how much these violations affect actual performance.

---

### ðŸŸ  1.4 MAJOR: Monte Carlo Verification is Circular

**Location:** `src/theory/theorem_verification.py` (lines 80-108)

**Problem:** The verification simulates a **fake** bandit environment:
```python
# True arm means (unknown to bandit)
arm_means = np.random.uniform(0.3, 0.8, size=self.n_arms)
```

This does NOT verify that bounds hold on your **actual** LLM debiasing task. It only verifies LinUCB works on synthetic linear bandits (which is already known).

---

## 2. EXPERIMENTAL DESIGN CONCERNS

### ðŸ”´ 2.1 CRITICAL: Reward Signal Contamination (MAJOR LOOPHOLE)

**Location:** `src/reward/bias_scorer.py`, `src/reward/quality_scorer.py`, and `src/context_extractor/context_encoder.py`

**Problem:** The same embedding model (`paraphrase-multilingual-MiniLM-L12-v2`) is used for:
1. **Bias scoring** (in `bias_scorer.py`)
2. **Quality scoring** (in `quality_scorer.py`)  
3. **Context encoding** (in `context_encoder.py`)

**Critical Loophole:** This creates **information leakage**:
- The bandit learns to exploit quirks of the embedding model, not actual bias
- Improvements in "bias score" may reflect overfitting to embedding space
- The context vector already contains information about what the reward will be

**This is a fundamental methodological flaw that could invalidate your core claims.**

**Recommendation:** 
```python
# Use DIFFERENT models for each component:
# - Context: 'paraphrase-multilingual-MiniLM-L12-v2' (current)
# - Bias scoring: 'cardiffnlp/twitter-roberta-base-sentiment' or similar
# - Quality scoring: BERTScore with different backbone

# OR at minimum, use different layers/pooling strategies
```

---

### ðŸ”´ 2.2 CRITICAL: Steering Vector Training Data Overlap

**Location:** `scripts/create_steering_vectors.py` and `data/steering_vectors/`

**Critical Question:** Are steering vectors trained on the SAME datasets (Multi-CrowS-Pairs, IndiBias) that you use for evaluation?

**Current code shows:**
```python
pairs_file = data_dir / f"{bias_type}_pairs.json"
```

If these pairs overlap with your evaluation data â†’ **Results are invalid** due to train-test contamination.

**Recommendation:** Explicitly document:
1. What data was used to create steering vectors
2. Ensure ZERO overlap with evaluation data
3. Use separate splits or entirely different datasets (e.g., WinoBias for steering, CrowS-Pairs for evaluation)

---

### ðŸŸ  2.3 MAJOR: Evaluation Metric Computes Biased Estimates

**Location:** `src/metrics/ibr.py` (lines 60-80)

**Problem:** IBR computes bias reduction as:
```python
reduction = (baseline_bias - method_bias) / baseline_bias
```

**Issues:**
1. If `baseline_bias â‰ˆ 0` (low bias input), you get division instability
2. Negative reduction (method increases bias) is clipped: `np.clip(reduction, -1.0, 1.0)`

**Your fix (signed_ibr) is good but incomplete:**
```python
def compute_signed_ibr(bias_reductions, epsilon=1e-10):
    # This penalizes worsened categories
    # But standard ibr_score still ignores them!
```

**Loophole:** A method that **increases** bias in some categories but reduces in others can still get high `ibr_score` because negative reductions are excluded from harmonic mean.

**Recommendation:** 
- Always report BOTH `ibr_score` AND `signed_ibr` 
- Add explicit count of categories where bias WORSENED
- Report worst-case category performance prominently

---

### ðŸŸ  2.4 MAJOR: Model Mismatch Between README and Prompts

**README.md originally stated:**
- Qwen 2.5 **7B**, Aya Expanse **8B**, Llama 3.1 **8B**

**Current code uses:**
- Qwen2.5-**1.5B**-Instruct, Llama-3.2-**1B**-Instruct, Gemma-2-**2B**-it

**Impact:** This is now consistent in the updated README, but ensure all documentation matches.

---

### ðŸŸ¡ 2.5 MODERATE: Warmup Strategy May Bias Arm Selection

**Location:** `src/pipeline/sequential_training_pipeline.py`

**Problem:** During warmup, you likely use uniform random arm selection. However:
- Some arms (steering vectors) have higher computational cost
- If warmup is too short, bandit may not explore all arms sufficiently
- If warmup is too long, you waste samples

**Recommendation:** Report arm selection frequency during AND after warmup to ensure fair exploration.

---

### ðŸŸ¡ 2.6 MODERATE: Output Adjustment Arm is English-Only

**Location:** `src/debiasing_arms/output_adjustment_arm.py`

The token adjustments are English-only:
```python
'he': -2.0, 'she': -2.0, 'they': +1.0
```

**Problem:** This arm will NOT work for Hindi/Bengali inputs. This means:
- You effectively have 6 arms for English but only 5 functional arms for Hindi/Bengali
- This confounds your cross-lingual analysis

**Recommendation:** Either:
1. Add Hindi/Bengali token mappings
2. Disable this arm for non-English languages
3. Report results separately showing this limitation

---

## 3. RESULT REGISTRATION CONCERNS

### ðŸ”´ 3.1 CRITICAL: Results Will NOT Be Properly Registered Without These Changes

**Problem 1: Counterfactual Rewards**
- Your regret tracking requires `optimal_reward` but this is rarely available
- Results will show **estimated** regret, not **true** regret

**Problem 2: Per-Sample Arm Performance**
- To claim "adaptive > static", you need to run ALL arms on evaluation samples
- Currently, you only run the SELECTED arm

**Problem 3: Statistical Significance**
- You HAVE implemented statistical tests in `src/metrics/statistical_tests.py` âœ“
- **Ensure these are ALWAYS reported** in CSV outputs

**REQUIRED for valid results:**
```python
# For EACH evaluation sample:
for sample in eval_data:
    results_per_arm = {}
    for arm in range(6):
        output = generate_with_arm(model, sample, arm)
        results_per_arm[arm] = {
            'bias_score': compute_bias(output),
            'quality_score': compute_quality(output),
            'reward': compute_reward(output)
        }
    
    selected_arm = bandit.select_arm(context)
    optimal_arm = argmax(results_per_arm, key='reward')
    
    # NOW you have true counterfactual data for regret computation
```

Your `CounterfactualEvaluator` does this! **Make sure it's used.**

---

### ðŸŸ  3.2 MAJOR: CSV Output Missing Critical Columns

**Location:** Output CSV format

**Required columns for valid research:**
1. `Confidence_Interval_95_Lower` - for all metrics âœ“ (implemented)
2. `Confidence_Interval_95_Upper` - for all metrics âœ“ (implemented)
3. `Statistical_Significance_P_Value` - for comparisons âœ“ (implemented)
4. `Number_Of_Samples` - per configuration
5. `Random_Seed` - for reproducibility
6. `Counterfactual_Rewards_Computed` - boolean flag
7. `N_Categories_Worsened` - for IBR transparency

---

## 4. NEW CONCERNS IDENTIFIED

### ðŸ”´ 4.1 CRITICAL: Reward Function Creates Perverse Incentives

**Location:** `src/reward/reward_calculator.py`

**Current formula:**
```python
reward = (1 - bias_score) * bias_weight + quality_score * quality_weight
# Default: bias_weight=0.6, quality_weight=0.4
```

**Problem:** This creates a **quality floor problem**:
- An arm that outputs NOTHING (empty string) might score:
  - bias_score â‰ˆ 0 (no biased content)
  - quality_score â‰ˆ 0 (poor quality)
  - reward = 0.6 * 1.0 + 0.4 * 0.0 = 0.6

**A degenerate solution (output nothing) gets reward 0.6!**

**Recommendation:**
```python
# Add quality floor requirement
if quality_score < 0.3:  # Minimum quality threshold
    reward = quality_score * 0.1  # Heavy penalty
else:
    reward = (1 - bias_score) * bias_weight + quality_score * quality_weight
```

---

### ðŸ”´ 4.2 CRITICAL: Context-Reward Correlation May Be Spurious

**Location:** `src/context_extractor/context_encoder.py`

**Problem:** Your context includes:
```python
# From encode():
- language_features (one-hot)
- demographic_features (detected demographics)
- topic_features (topic probabilities)
- bias_risk (estimated bias risk)
- text_embedding (from same model as bias scorer!)
```

The `bias_risk` feature and `text_embedding` are **predictive of the reward** by construction. This means:
- The bandit may learn to select arms based on predicted reward, not actual arm effectiveness
- This could lead to **correct arm selection for wrong reasons**

**Recommendation:** Remove `bias_risk` from context OR verify it doesn't correlate with arm optimality.

---

### ðŸŸ  4.3 MAJOR: Hook Cleanup in Steering Vector Arm

**Location:** `src/debiasing_arms/steering_vector_arm.py`

**Verification needed:**
```python
self.active_hooks = []
# Hook removal in _remove_hooks() called in finally block âœ“
```

**Good:** You do have cleanup. But verify:
1. What happens if generation raises an exception?
2. Are hooks properly removed in all error cases?
3. Memory profiling to ensure no leaks over many samples

---

### ðŸŸ¡ 4.4 MODERATE: Topic Classifier Uses Large External Model

**Location:** `src/context_extractor/topic_classifier.py`

```python
model="facebook/bart-large-mnli"
```

**Problem:** This is a 407M parameter model loaded alongside your LLM. With 24GB VRAM constraint, this may cause issues with larger LLMs.

**Recommendation:** Consider smaller zero-shot classifier or distilled model.

---

### ðŸŸ¡ 4.5 MODERATE: Regret Tracking Clips to Non-Negative

**Location:** `src/theory/regret_tracker.py` (line ~119)

```python
self.cumulative_regret += max(0, inst_regret)
```

**Problem:** Negative regret (selected arm better than estimated optimal) is clipped to 0. This:
- Hides information about when the algorithm "gets lucky"
- May underestimate true regret variance

**Recommendation:** Track both clipped and unclipped regret for analysis.

---

## 5. RECOMMENDED EXPERIMENTS TO ADD

### 5.1 Required Baseline Comparisons

| Experiment | Purpose | Status |
|------------|---------|--------|
| **Random Arm Selection** | Verify bandit learns better than random | Required |
| **Best Static Arm (Oracle)** | Requires full counterfactual data | Required |
| **Per-Category Best Static** | Different optimal arm per bias type? | Recommended |
| **Prompt-Only Baseline** | Is steering necessary? | Recommended |
| **No Debiasing Baseline** | What's the actual harm of bias? | Required |

### 5.2 Ablation Studies Missing

| Ablation | Question | Priority |
|----------|----------|----------|
| Context dimension | Is 128-dim necessary? Try 32, 64, 256 | Medium |
| Steering strength | Vary strength from 0.1 to 2.0 | High |
| Fairness threshold | Test Ï„ âˆˆ {0.1, 0.2, 0.3, 0.4, 0.5} | High |
| Lambda weight | Test Î» âˆˆ {0.0, 0.25, 0.5, 0.75, 1.0} | High |
| Reward weights | Test bias_weight âˆˆ {0.4, 0.5, 0.6, 0.7, 0.8} | Medium |
| Layer range for steering | Which layers matter most? | Medium |

### 5.3 Cross-Validation Required

- **K-Fold CV**: Report mean Â± std across 5 folds
- **Separate test set**: Hold out 20% completely unseen data
- **Multiple random seeds**: Run each experiment 3-5 times

---

## 6. PUBLICATION READINESS CHECKLIST

### Before Submitting to TACL:

- [ ] **Counterfactual evaluation**: Run all 6 arms on all eval samples
- [ ] **True regret computation**: Use actual optimal rewards
- [ ] **Statistical tests**: Add paired t-tests or bootstrap CIs âœ“ (implemented)
- [ ] **Steering vector documentation**: Prove no train-test overlap
- [ ] **Model consistency**: Fix 7B/8B vs 1.5B/2B discrepancy âœ“ (updated)
- [ ] **Reproducibility**: Fixed seeds, version numbers, hardware specs
- [ ] **Negative results**: Report where method FAILS (which categories, languages)
- [ ] **Error analysis**: Manual inspection of 50+ failure cases
- [ ] **Human evaluation**: At minimum, verify 100 samples manually
- [ ] **Separate embedding models**: Use different models for context vs reward
- [ ] **Quality floor**: Add minimum quality requirement to reward
- [ ] **Output adjustment multilingual**: Fix or document limitation
- [ ] **Reward formula**: Document and justify weight choices

---

## 7. POSITIVE ASPECTS

Despite the concerns, the research has significant strengths:

1. âœ… **Novel metrics** (IBR, FAR) are interesting contributions
2. âœ… **Multilingual focus** (English, Hindi, Bengali) is underexplored
3. âœ… **Caste bias** inclusion is valuable for Indian context
4. âœ… **Code organization** is clean and modular
5. âœ… **Multiple bandit algorithms** (LinUCB, Thompson, Neural) good comparison
6. âœ… **Statistical testing infrastructure** is well-implemented
7. âœ… **CounterfactualEvaluator** addresses the oracle regret problem
8. âœ… **signed_ibr** addresses the negative reduction problem
9. âœ… **theoretical_limitations.md** shows good scientific transparency

---

## 8. SUMMARY OF REQUIRED CHANGES

| Priority | Issue | Effort | Impact |
|----------|-------|--------|--------|
| ðŸ”´ CRITICAL | Use different embedding models for context vs reward | HIGH | Validity |
| ðŸ”´ CRITICAL | Verify steering vector train-test split | LOW | Validity |
| ðŸ”´ CRITICAL | Always use CounterfactualEvaluator for results | MEDIUM | Validity |
| ðŸ”´ CRITICAL | Add quality floor to reward function | LOW | Correctness |
| ðŸŸ  MAJOR | Document all theoretical assumption violations | MEDIUM | Transparency |
| ðŸŸ  MAJOR | Add required baseline comparisons | HIGH | Claims |
| ðŸŸ  MAJOR | Report signed_ibr alongside ibr_score | LOW | Transparency |
| ðŸŸ¡ MODERATE | Fix output adjustment for non-English | MEDIUM | Coverage |
| ðŸŸ¡ MODERATE | Add ablation studies | HIGH | Understanding |
| ðŸŸ¡ MODERATE | Multiple random seeds | MEDIUM | Robustness |

---

## 9. WILL RESULTS BE REGISTERED PROPERLY?

**Current State:** Results will NOT be properly registered due to:
1. âŒ Regret computation uses estimated optimal (during training)
2. âŒ Same embedding model used for context and reward
3. âŒ Steering vector data provenance unclear
4. âš ï¸ CounterfactualEvaluator exists but may not be used everywhere

**Path Forward:** 
1. **MANDATORY**: Use CounterfactualEvaluator for ALL reported metrics
2. **MANDATORY**: Separate embedding models for context vs reward
3. **MANDATORY**: Document steering vector data source (no overlap)
4. **MANDATORY**: Add quality floor to reward
5. **RECOMMENDED**: Run ablations with multiple seeds

**Estimated Additional Work:** 3-6 weeks of engineering + experiments

---

## 10. CONCLUSION

The Fair-CB research direction is **sound and valuable**. The core idea of using contextual bandits for adaptive debiasing strategy selection is novel and promising. However, the current implementation has **critical methodological issues** that must be addressed:

1. **Information leakage** through shared embedding models
2. **Unverified train-test separation** for steering vectors
3. **Estimated vs true regret** (fixable with existing CounterfactualEvaluator)
4. **Perverse incentives** in reward function (quality floor needed)

**With these fixes, this could be a strong TACL submission.**

---

*This analysis is provided to strengthen the research. The concerns are addressable, and the research team has already implemented many good practices (statistical testing, counterfactual evaluation, limitation documentation). Addressing the remaining issues will make the published results scientifically sound and reproducible.*