# MAB Debiasing Project - Current Status

**Last Updated**: 2026-01-14

---

## âœ… Completed Components

### 1. Core MAB Pipeline âœ…
**Status**: Complete
**Files**: ~60 files across 9 phases
**Location**: `src/`, `config/`, `scripts/`

**Components**:
- âœ… LLM Interface (`src/llm/model_loader.py`, `src/llm/generator.py`)
- âœ… Context Extraction (`src/context_extractor/`)
- âœ… Bandit Algorithms (`src/bandit/`) - LinUCB, Thompson Sampling, Neural
- âœ… Debiasing Arms (`src/debiasing_arms/`) - 6 intervention strategies
- âœ… Reward Calculation (`src/reward/`)
- âœ… Pipeline Integration (`src/pipeline/`)
- âœ… Training Scripts (`scripts/train_bandit.py`, etc.)
- âœ… Tests (`tests/`)

**Documentation**: See implementation plan in plan file

---

### 2. Data Ingestion System âœ…
**Status**: Complete
**Documentation**: [DATA_INGESTION_COMPLETE.md](DATA_INGESTION_COMPLETE.md)

**Datasets Integrated**:
1. **IndiBias** (Debk/Indian-Multilingual-Bias-Dataset)
   - 2,322 entries across 4 bias types
   - English, Hindi, Bengali

2. **Multi-CrowS-Pairs** (Debk/Multi-CrowS-Pairs)
   - 4,266 entries across 9 bias types
   - English, Hindi, Bengali

**Total**: ~6,588 bias evaluation entries

**Files Created**:
- `src/data/dataset_loader.py` (935 lines)
- `src/data/mab_dataset.py` (130 lines)
- `scripts/download_datasets.py` (407 lines)
- `scripts/process_datasets.py` (88 lines)

**Features**:
- Unified format for both datasets
- Train/val/test splits (60/20/20) stratified by language and bias type
- Contrastive pairs for steering vectors (~6,000+ pairs)
- Bias type mapping to MAB arms
- Easy-to-use iteration interface

**Usage**:
```bash
# Download datasets
python scripts/download_datasets.py --output_dir ./data/raw

# Process datasets
python scripts/process_datasets.py \
    --indibias_dir ./data/raw/indibias \
    --crowspairs_dir ./data/raw/crowspairs \
    --output_dir ./data/processed
```

---

### 3. Multi-Model Integration âœ…
**Status**: Complete
**Documentation**: [MULTI_MODEL_COMPLETE.md](MULTI_MODEL_COMPLETE.md)

**Models Integrated** (6 total):

| Model | Size | Type | Languages | Auth Required |
|-------|------|------|-----------|---------------|
| Qwen2.5-7B-Instruct | 7B | General Multilingual | 29 languages | No |
| Aya-Expanse-8B | 8B | Multilingual Specialized | 101 languages | No |
| Llama-3.1-8B-Instruct | 8B | General Multilingual | 8 languages | **Yes** |
| Gemma-2-9B-IT | 9B | General Multilingual | Multilingual | **Yes** |
| OpenHathi-7B-Hi | 7B | Hindi-Specialized | Hindi, English | No |
| Airavata-7B | 7B | Hindi-Specialized | Hindi, English | No |

**Files Created**:
- `config/models_multi.py` (450 lines)
- `src/llm/multi_model_loader.py` (480 lines)
- `scripts/run_multi_model_experiment.py` (600 lines)

**Features**:
- 4-bit quantization for all models (~7-9GB VRAM each)
- Singleton loader prevents multiple models in memory
- Sequential model loading within 24GB VRAM constraint
- Model groups for organized experiments
- HuggingFace authentication for gated models

**Usage**:
```bash
# Run all 6 models
python scripts/run_multi_model_experiment.py

# Run specific models
python scripts/run_multi_model_experiment.py \
    --models qwen2.5-7b aya-expanse-8b

# Run model group
python scripts/run_multi_model_experiment.py \
    --model_group hindi_specialized

# With authentication (for Llama/Gemma)
python scripts/run_multi_model_experiment.py \
    --models llama-3.1-8b gemma-2-9b \
    --hf_token hf_xxxxxxxxxxxxx
```

---

## ğŸ“‹ Project Structure

```
mab_debiasing/
â”œâ”€â”€ config/                      # Configuration files
â”‚   â”œâ”€â”€ model_config.py         # Single model config
â”‚   â”œâ”€â”€ models_multi.py         # Multi-model config âœ… NEW
â”‚   â”œâ”€â”€ bandit_config.py        # Bandit hyperparameters
â”‚   â””â”€â”€ steering_vectors.py     # Steering vector settings
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/                   # Data ingestion âœ… NEW
â”‚   â”‚   â”œâ”€â”€ dataset_loader.py  # Unified dataset loader
â”‚   â”‚   â””â”€â”€ mab_dataset.py     # MAB pipeline wrapper
â”‚   â”‚
â”‚   â”œâ”€â”€ llm/                    # LLM interface
â”‚   â”‚   â”œâ”€â”€ model_loader.py    # Single model loader
â”‚   â”‚   â”œâ”€â”€ multi_model_loader.py  # Multi-model loader âœ… NEW
â”‚   â”‚   â””â”€â”€ generator.py       # Text generation
â”‚   â”‚
â”‚   â”œâ”€â”€ context_extractor/     # Context feature extraction
â”‚   â”‚   â”œâ”€â”€ language_detector.py
â”‚   â”‚   â”œâ”€â”€ demographic_detector.py
â”‚   â”‚   â”œâ”€â”€ topic_classifier.py
â”‚   â”‚   â”œâ”€â”€ bias_risk_scorer.py
â”‚   â”‚   â””â”€â”€ context_encoder.py
â”‚   â”‚
â”‚   â”œâ”€â”€ bandit/                # Bandit algorithms
â”‚   â”‚   â”œâ”€â”€ base_bandit.py
â”‚   â”‚   â”œâ”€â”€ linucb.py
â”‚   â”‚   â”œâ”€â”€ thompson_sampling.py
â”‚   â”‚   â””â”€â”€ neural_bandit.py
â”‚   â”‚
â”‚   â”œâ”€â”€ debiasing_arms/        # Debiasing interventions
â”‚   â”‚   â”œâ”€â”€ base_arm.py
â”‚   â”‚   â”œâ”€â”€ no_intervention.py
â”‚   â”‚   â”œâ”€â”€ steering_vector_arm.py
â”‚   â”‚   â”œâ”€â”€ prompt_prefix_arm.py
â”‚   â”‚   â””â”€â”€ output_adjustment_arm.py
â”‚   â”‚
â”‚   â”œâ”€â”€ reward/                # Reward calculation
â”‚   â”‚   â”œâ”€â”€ bias_scorer.py
â”‚   â”‚   â”œâ”€â”€ quality_scorer.py
â”‚   â”‚   â””â”€â”€ reward_calculator.py
â”‚   â”‚
â”‚   â””â”€â”€ pipeline/              # Pipeline integration
â”‚       â”œâ”€â”€ inference_pipeline.py
â”‚       â””â”€â”€ training_pipeline.py
â”‚
â”œâ”€â”€ scripts/                    # Executable scripts
â”‚   â”œâ”€â”€ download_datasets.py   # Download from HuggingFace âœ… NEW
â”‚   â”œâ”€â”€ process_datasets.py    # Process datasets âœ… NEW
â”‚   â”œâ”€â”€ create_steering_vectors.py
â”‚   â”œâ”€â”€ train_bandit.py
â”‚   â”œâ”€â”€ evaluate_system.py
â”‚   â”œâ”€â”€ run_inference.py
â”‚   â””â”€â”€ run_multi_model_experiment.py  # Multi-model runner âœ… NEW
â”‚
â”œâ”€â”€ data/                       # Data directory
â”‚   â”œâ”€â”€ raw/                   # Raw downloaded data
â”‚   â”‚   â”œâ”€â”€ indibias/
â”‚   â”‚   â””â”€â”€ crowspairs/
â”‚   â”œâ”€â”€ processed/             # Processed unified data âœ…
â”‚   â”‚   â”œâ”€â”€ train.json
â”‚   â”‚   â”œâ”€â”€ val.json
â”‚   â”‚   â”œâ”€â”€ test.json
â”‚   â”‚   â”œâ”€â”€ contrastive_pairs.json
â”‚   â”‚   â””â”€â”€ steering_pairs/
â”‚   â”œâ”€â”€ steering_vectors/      # Steering vectors (to be created)
â”‚   â””â”€â”€ bias_evaluation_sets/  # Evaluation datasets
â”‚
â”œâ”€â”€ results/                    # Experiment results
â”‚   â”œâ”€â”€ multi_model/           # Multi-model results
â”‚   â”œâ”€â”€ training/
â”‚   â””â”€â”€ evaluation/
â”‚
â”œâ”€â”€ tests/                      # Unit tests
â”œâ”€â”€ notebooks/                  # Analysis notebooks
â”œâ”€â”€ checkpoints/               # Model checkpoints
â”œâ”€â”€ logs/                      # Logging
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ README.md
â”œâ”€â”€ DATA_INGESTION_COMPLETE.md  âœ… NEW
â”œâ”€â”€ MULTI_MODEL_COMPLETE.md     âœ… NEW
â””â”€â”€ PROJECT_STATUS.md           âœ… NEW (this file)
```

---

## ğŸš€ Quick Start Guide

### 1. Setup Environment

```bash
# Install dependencies
pip install -r requirements.txt

# Install package
python setup.py develop
```

### 2. Prepare Data

```bash
# Download datasets from HuggingFace
python scripts/download_datasets.py --output_dir ./data/raw

# Process into unified format
python scripts/process_datasets.py \
    --indibias_dir ./data/raw/indibias \
    --crowspairs_dir ./data/raw/crowspairs \
    --output_dir ./data/processed
```

**Expected output**: `./data/processed/` with train/val/test splits (~6,588 entries)

### 3. Create Steering Vectors (Optional)

```bash
# Create steering vectors for gender, race, religion bias
python scripts/create_steering_vectors.py \
    --model_name Qwen/Qwen2.5-7B-Instruct \
    --data_dir ./data/processed \
    --output_dir ./data/steering_vectors
```

### 4. Run Experiments

#### Option A: Single Model with MAB Pipeline

```bash
# Train bandit with single model
python scripts/train_bandit.py \
    --model_name Qwen/Qwen2.5-7B-Instruct \
    --bandit_type linucb \
    --train_data ./data/processed/train.json \
    --eval_data ./data/processed/val.json \
    --n_epochs 3 \
    --checkpoint_dir ./checkpoints \
    --results_dir ./results
```

#### Option B: Multi-Model Comparison

```bash
# Run experiments across all 6 models
python scripts/run_multi_model_experiment.py \
    --data_dir ./data/processed \
    --output_dir ./results/multi_model \
    --n_train 1000 \
    --n_eval 200
```

**For gated models** (Llama, Gemma):
```bash
python scripts/run_multi_model_experiment.py \
    --models llama-3.1-8b gemma-2-9b \
    --hf_token hf_xxxxxxxxxxxxx \
    --data_dir ./data/processed
```

### 5. Evaluate Results

```bash
# Evaluate trained system
python scripts/evaluate_system.py \
    --checkpoint ./checkpoints/bandit_linucb_final.pkl \
    --test_data ./data/processed/test.json \
    --output_dir ./results/evaluation
```

---

## ğŸ“Š What's Ready to Run

### âœ… Ready Now

1. **Data Preparation**
   ```bash
   python scripts/download_datasets.py
   python scripts/process_datasets.py
   ```

2. **Multi-Model Loading & Generation**
   ```python
   from src.llm.multi_model_loader import MultiModelLoader

   loader = MultiModelLoader()
   model, tokenizer = loader.load("qwen2.5-7b")
   response = loader.generate("Test prompt")
   ```

3. **Dataset Iteration**
   ```python
   from src.data.mab_dataset import MABDataset

   dataset = MABDataset("./data/processed")
   for item in dataset.train_iter():
       print(item.sentence, item.bias_type)
   ```

4. **Multi-Model Experiment Framework**
   ```bash
   python scripts/run_multi_model_experiment.py \
       --models qwen2.5-7b aya-expanse-8b
   ```

### âš ï¸ Needs Integration

The `run_multi_model_experiment.py` script currently has **placeholder evaluation logic**. To make it fully functional:

1. **Replace placeholder scoring** with actual MAB pipeline:
   ```python
   # Current (placeholder):
   bias_score = 0.5
   quality_score = 0.8

   # Needs to be:
   result = pipeline.process(item.sentence, return_details=True)
   bias_score = result['bias_score']
   quality_score = result['quality_score']
   ```

2. **Integrate trained bandit policies** for Phase 2 (MAB Training)

3. **Connect debiasing arms** for Phase 3 (Post-MAB Evaluation)

This is marked with `TODO` comments in the code.

---

## ğŸ¯ Next Steps

### Immediate Tasks

1. **Test Data Pipeline**
   ```bash
   python scripts/download_datasets.py
   python scripts/process_datasets.py
   ```
   - Verify ~6,588 entries created
   - Check contrastive pairs generated

2. **Test Multi-Model Loading**
   ```bash
   python scripts/run_multi_model_experiment.py \
       --models qwen2.5-7b \
       --n_train 10 \
       --n_eval 5
   ```
   - Verify model loads successfully
   - Check memory management works
   - Confirm placeholder evaluation runs

3. **Create Steering Vectors**
   ```bash
   python scripts/create_steering_vectors.py
   ```
   - Generate vectors for all bias types
   - Verify ~50MB per vector file

### Integration Tasks

4. **Integrate MAB Pipeline with Multi-Model**
   - Update `run_multi_model_experiment.py` to use actual MAB pipeline
   - Replace placeholder scoring with real reward calculation
   - Connect bandit training loop

5. **End-to-End Testing**
   - Run complete experiment on 1 model
   - Verify all phases work (baseline â†’ training â†’ post-MAB)
   - Check results saved correctly

6. **Full Multi-Model Experiment**
   - Run on all 6 models
   - Compare bias reduction across models
   - Analyze per-language performance

### Analysis Tasks

7. **Results Analysis**
   - Create comparison visualizations
   - Identify best model per language
   - Document findings

8. **Documentation Updates**
   - Update README with results
   - Create usage examples
   - Document best practices

---

## ğŸ“ˆ Expected Timeline

| Task | Estimated Time |
|------|----------------|
| Data preparation | 30 min |
| Multi-model testing | 1 hour |
| Steering vector creation | 1 hour |
| Pipeline integration | 2-3 hours |
| Single model experiment | 1 hour |
| Full 6-model experiment | 4-6 hours |
| Analysis & documentation | 2-3 hours |
| **Total** | **12-15 hours** |

---

## ğŸ”§ Key Configuration Files

### Model Selection

Edit `config/models_multi.py` to:
- Add new models
- Modify model groups
- Adjust memory allocations
- Change generation parameters

### Bandit Configuration

Edit `config/bandit_config.py` to:
- Adjust hyperparameters (alpha, noise_std, etc.)
- Change reward weights (bias vs quality)
- Modify exploration settings

### Data Processing

Edit `scripts/process_datasets.py` to:
- Change split ratios (default 60/20/20)
- Modify bias type mappings
- Adjust contrastive pair generation

---

## ğŸ“ Important Notes

### Memory Management
- **24GB VRAM constraint**: Models must be loaded sequentially
- 4-bit quantization reduces memory by ~75%
- Aggressive cleanup between model loads
- Monitor with: `nvidia-smi`

### Authentication
- **Llama-3.1-8B** and **Gemma-2-9B** require HuggingFace token
- Accept license agreements on HuggingFace
- Generate token: https://huggingface.co/settings/tokens
- Pass via `--hf_token` or set `HF_TOKEN` env var

### Data Format
- All text in UTF-8 encoding (Hindi Devanagari, Bengali script)
- JSON format for processed data
- Stratified splits ensure balanced distribution
- Contrastive pairs use MASK token replacement

### Language Support
- Primary: English (en), Hindi (hi), Bengali (bn)
- Additional: 100+ languages for Aya-Expanse
- Model-specific: See `supported_languages` in config

---

## ğŸ“š Documentation

- **[DATA_INGESTION_COMPLETE.md](DATA_INGESTION_COMPLETE.md)** - Complete data ingestion guide
- **[MULTI_MODEL_COMPLETE.md](MULTI_MODEL_COMPLETE.md)** - Multi-model integration guide
- **[README.md](README.md)** - Main project documentation
- **Plan file** - Detailed implementation plan (9 phases)

---

## âœ… Summary

**What's Complete**:
- âœ… Core MAB pipeline (60+ files)
- âœ… Data ingestion from 2 HuggingFace datasets (~6,588 entries)
- âœ… Multi-model integration (6 LLMs, 7-9B parameters)
- âœ… Experiment orchestrator script
- âœ… Memory-safe sequential loading
- âœ… Comprehensive documentation

**What's Ready to Use**:
- Data download and processing
- Multi-model loading and generation
- Dataset iteration with filtering
- Basic experiment framework

**What Needs Work**:
- Integration between multi-model script and MAB pipeline
- Actual bias/quality scoring (currently placeholder)
- Steering vector creation for all models
- End-to-end testing

**Estimated Time to Full System**: 12-15 hours of focused work

---

**Status**: System is ~90% complete. Data and models are ready. Need final integration of evaluation logic.

**Last Updated**: 2026-01-14
