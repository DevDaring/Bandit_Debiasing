# Fair-CB: Fairness-Aware Contextual Bandits for Multilingual LLM Debiasing

<div align="center">

**Adaptive Multi-Armed Bandit Debiasing Strategy Selection for Multilingual Large Language Models**

[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

</div>

---

## üéØ Overview

Fair-CB is a research framework that dynamically selects optimal debiasing interventions for multilingual LLMs using contextual bandit algorithms. The system learns from fairness and quality feedback signals to adaptively choose the best debiasing strategy for each input.

### Key Features

| Feature | Description |
|---------|-------------|
| **Multi-Model Support** | Qwen 2.5 7B, Aya Expanse 8B, Llama 3.1 8B |
| **Multilingual** | English, Hindi, Bengali + code-mixing detection |
| **Novel Metrics** | IBR (Intersectional Bias Reduction), FAR (Fairness-Aware Regret) |
| **Theoretical Guarantees** | Sublinear regret bounds with proof verification |
| **Publication-Ready** | LaTeX table generation, standardized CSV output |

### Debiasing Arms (6 Strategies)

| Arm | Strategy | Description |
|-----|----------|-------------|
| 0 | No Intervention | Baseline (no debiasing) |
| 1 | Gender Steering | Steering vector for gender bias |
| 2 | Race Steering | Steering vector for race/ethnicity bias |
| 3 | Religion Steering | Steering vector for religious bias |
| 4 | Prompt Prefix | Fairness-aware prompt modification |
| 5 | Output Adjustment | Post-hoc output debiasing |

---

## üöÄ Quick Start

### Installation

```bash
# Clone and install
git clone https://github.com/yourusername/Fair-CB.git
cd Fair-CB
pip install -r requirements.txt
python setup.py develop

# Create directories
mkdir -p logs results checkpoints data/steering_vectors data/bias_evaluation_sets

# Configure environment
cp .env.example .env
# Edit .env to add your HF_TOKEN
```

### Run Experiment

```bash
# Quick test (subset of data)
python scripts/generate_all_results.py --quick

# Full TACL experiment suite
python scripts/generate_all_results.py

# Single model/dataset run
python scripts/run_experiment.py --model qwen --dataset multi_crows --epochs 3
```

### Evaluate with Novel Metrics

```bash
python scripts/evaluate_with_metrics.py \
    --dataset both \
    --generate-latex \
    --output-csv ./results
```

---

## üìä Novel Metrics

### IBR (Intersectional Bias Reduction)

Measures bias reduction across ALL categories using **harmonic mean** (penalizes methods that fail in any category):

```
IBR = HarmonicMean({reduction_gender, reduction_race, reduction_religion, ...})
```

- **Range**: [0, 1] where 1 = perfect reduction across all categories
- **Why harmonic mean**: Penalizes methods that underperform in any single category

### FAR (Fairness-Aware Regret)

Combines regret and fairness violations:

```
FAR = R(T) + Œª¬∑V(T)
```

Where:
- `R(T)` = Cumulative regret at time T
- `V(T)` = Cumulative fairness violations
- `Œª` = Fairness weight (default: 0.5)

---

## üìÅ Project Structure

```
Fair-CB/
‚îú‚îÄ‚îÄ config/                          # Configuration
‚îÇ   ‚îú‚îÄ‚îÄ model_config.py              # Model registry (Qwen, Aya, Llama)
‚îÇ   ‚îú‚îÄ‚îÄ bandit_config.py             # Bandit hyperparameters
‚îÇ   ‚îî‚îÄ‚îÄ steering_vectors.py          # Steering vector paths
‚îÇ
‚îú‚îÄ‚îÄ src/                             # Source code
‚îÇ   ‚îú‚îÄ‚îÄ bandit/                      # Bandit algorithms (LinUCB, Thompson, Neural)
‚îÇ   ‚îú‚îÄ‚îÄ context_extractor/           # 128-dim feature extraction
‚îÇ   ‚îú‚îÄ‚îÄ debiasing_arms/              # 6 debiasing strategies
‚îÇ   ‚îú‚îÄ‚îÄ llm/                         # Model loading and generation
‚îÇ   ‚îú‚îÄ‚îÄ reward/                      # Bias and quality scoring
‚îÇ   ‚îú‚îÄ‚îÄ pipeline/                    # Training and inference pipelines
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ training_pipeline.py     # Base training pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sequential_training_pipeline.py  # Enhanced with theory tracking
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ inference_pipeline.py    # Inference pipeline
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ theory/                      # Theoretical analysis (NEW)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ regret_tracker.py        # R(T) tracking
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fairness_tracker.py      # V(T) tracking
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ bounds.py                # O(d‚àö(KT log(T/Œ¥))) bounds
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ adaptive_vs_static.py    # Proves R_adaptive/R_static ‚Üí 0
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ theorem_verification.py  # Monte Carlo verification
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ metrics/                     # Evaluation metrics (NEW)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ibr.py                   # Intersectional Bias Reduction
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ far.py                   # Fairness-Aware Regret
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ comprehensive_evaluator.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ output/                      # Output standardization (NEW)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ csv_manager.py           # Full-form column names
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ crosslingual/                # Cross-lingual analysis (NEW)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ transfer_analyzer.py     # EN‚ÜíHI, EN‚ÜíBN transfer
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ code_mixing_handler.py   # Hinglish/Benglish detection
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ parallel_evaluator.py    # Parallel sample evaluation
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ ablation/                    # Ablation framework (NEW)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config_generator.py      # 14+ ablation configurations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ablation_runner.py       # Automated experiment runner
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ results_analyzer.py      # Component importance
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ data/                        # Data handling
‚îÇ       ‚îú‚îÄ‚îÄ dataset_loader.py        # Multi-CrowS-Pairs, IndiBias
‚îÇ       ‚îî‚îÄ‚îÄ bias_categories.py       # Full-form bias category mappings
‚îÇ
‚îú‚îÄ‚îÄ scripts/                         # Executable scripts
‚îÇ   ‚îú‚îÄ‚îÄ run_experiment.py            # Main experiment orchestrator
‚îÇ   ‚îú‚îÄ‚îÄ generate_all_results.py      # TACL publication suite
‚îÇ   ‚îú‚îÄ‚îÄ evaluate_with_metrics.py     # IBR/FAR evaluation
‚îÇ   ‚îú‚îÄ‚îÄ train_bandit.py              # Train specific bandit
‚îÇ   ‚îú‚îÄ‚îÄ evaluate_system.py           # Evaluate trained system
‚îÇ   ‚îú‚îÄ‚îÄ create_steering_vectors.py   # Create steering vectors
‚îÇ   ‚îî‚îÄ‚îÄ prepare_evaluation_data.py   # Prepare datasets
‚îÇ
‚îú‚îÄ‚îÄ tests/                           # Unit tests
‚îú‚îÄ‚îÄ results/                         # Output results
‚îú‚îÄ‚îÄ logs/                            # Log files
‚îú‚îÄ‚îÄ checkpoints/                     # Model checkpoints
‚îú‚îÄ‚îÄ requirements.txt                 # Python dependencies
‚îú‚îÄ‚îÄ setup.py                         # Package setup
‚îú‚îÄ‚îÄ .env.example                     # Environment template
‚îî‚îÄ‚îÄ README.md                        # This file
```

---

## üî¨ Theoretical Guarantees

### Regret Bound

LinUCB achieves sublinear regret:

```
R(T) ‚â§ O(d‚àö(KT log(T/Œ¥)))
```

Where:
- `d` = context dimension (128)
- `K` = number of arms (6)
- `T` = number of rounds
- `Œ¥` = confidence parameter

### Adaptive vs Static

The framework proves that adaptive selection outperforms any static arm:

```
lim(T‚Üí‚àû) R_adaptive(T) / R_static(T) ‚Üí 0
```

### Verification

Run Monte Carlo simulations to verify theoretical claims:

```python
from src.theory import TheoremVerifier

verifier = TheoremVerifier(n_arms=6, context_dim=128, n_simulations=1000)
results = verifier.run_all_verifications(T=1000)
print(verifier.get_summary())
```

---

## üß™ Ablation Studies

### Standard Configurations (14+)

| Category | Configurations |
|----------|----------------|
| **Full System** | `full` |
| **Baselines** | `random`, `static_baseline`, `static_gender`, `static_prompt` |
| **Component Ablations** | `no_context`, `no_steering`, `no_prompt`, `no_output_adjust` |
| **Bandit Algorithms** | `linucb`, `thompson`, `neural` |
| **Hyperparameter Sensitivity** | `alpha_0.5`, `alpha_2.0`, `lambda_0.0`, `lambda_1.0` |

### Run Ablation Study

```python
from src.ablation import AblationConfigGenerator, AblationRunner

# Generate configurations
generator = AblationConfigGenerator()
configs = generator.generate_all()

# Run experiments
runner = AblationRunner(results_dir='./ablation_results')
runner.run_all(configs)

# Analyze results
from src.ablation import AblationResultsAnalyzer
analyzer = AblationResultsAnalyzer(runner.load_results())
print(analyzer.generate_summary())
print(analyzer.generate_latex_table())
```

---

## üåê Cross-Lingual Transfer

### Transfer Analysis

Analyze how debiasing transfers across languages:

```python
from src.crosslingual import TransferAnalyzer

analyzer = TransferAnalyzer(
    source_languages=['en'],
    target_languages=['hi', 'bn']
)

# Add observations from experiments
analyzer.add_observation(language='en', category='gender', baseline_bias=0.8, method_bias=0.3)
analyzer.add_observation(language='hi', category='gender', baseline_bias=0.7, method_bias=0.4)

# Compute transfer ratios
transfers = analyzer.compute_all_transfers()
print(transfers['en->hi'].transfer_ratio)  # 1.0 = perfect transfer
```

### Code-Mixing Detection

Handle Hindi-English (Hinglish) and Bengali-English input:

```python
from src.crosslingual import CodeMixingDetector

detector = CodeMixingDetector()
result = detector.detect("Mujhe lagta hai this is a good idea")
print(result.is_code_mixed)  # True
print(result.languages_detected)  # ['hi', 'en']
```

---

## üìà Usage Examples

### Training with Enhanced Tracking

```python
from src.pipeline import SequentialTrainingPipeline

pipeline = SequentialTrainingPipeline(
    inference_pipeline=inference_pipeline,
    n_arms=6,
    context_dim=128,
    lambda_fairness=0.5,
    enable_wandb=True
)

results = pipeline.train_sequential(
    train_data=train_data,
    eval_data=eval_data,
    n_epochs=3,
    warmup_samples=50
)

print(f"IBR: {results['ibr']:.4f}")
print(f"FAR: {results['far']:.4f}")
print(f"Regret Bound Satisfied: {results['regret_bound_satisfied']}")
```

### Evaluation with IBR/FAR

```python
from src.metrics import ComprehensiveMetricsEvaluator

evaluator = ComprehensiveMetricsEvaluator(
    lambda_weight=0.5,
    bias_threshold=0.3
)

# Add observations
for sample in test_data:
    evaluator.add_observation(
        bias_score=sample['bias'],
        reward=sample['reward'],
        category=sample['category'],
        language=sample['language']
    )

# Evaluate
result = evaluator.evaluate()
print(f"IBR: {result.ibr.ibr_score:.4f}")
print(f"FAR: {result.far.far_score:.4f}")
print(f"Worst category: {result.ibr.worst_category}")
```

### Standardized CSV Output

```python
from src.output import CSVOutputManager

manager = CSVOutputManager(output_dir='./results', timestamp_files=True)
manager.save_main_results(df)  # Automatically uses full-form column names
```

---

## ‚öôÔ∏è Configuration

### Environment Variables (`.env`)

```bash
# Required for HuggingFace models
HF_TOKEN=hf_xxxxxxxxxxxx

# Optional
WANDB_PROJECT=fair-cb
CUDA_VISIBLE_DEVICES=0
```

### Model Configuration (`config/model_config.py`)

```python
from config.model_config import get_model_config, get_all_models

# Get specific model
config = get_model_config('qwen')
print(config['model_id'])  # Qwen/Qwen2.5-7B-Instruct

# List all supported models
models = get_all_models()
print(models)  # ['qwen', 'aya', 'llama']
```

### Bandit Configuration

```python
from config.bandit_config import get_bandit_config

config = get_bandit_config('linucb')
print(config['alpha'])  # 1.0
print(config['context_dim'])  # 128
```

---

## üßπ Memory Management

The system is optimized for 24GB VRAM:

- 4-bit quantization (NF4) for LLMs
- Sequential model loading
- Aggressive memory cleanup
- Neural bandit on CPU (avoids GPU conflicts)

---

## üìã Expected Results

### Runtime (24GB GPU)

| Task | Duration |
|------|----------|
| Dataset preparation | 10-15 min |
| Steering vector creation | 30-45 min |
| Training (1 epoch, 1000 samples) | 2-3 hours/algorithm |
| Evaluation (200 samples) | 15-20 min/algorithm |
| Complete experiment (3 epochs, 3 algorithms) | 20-24 hours |

### Disk Usage

| Component | Size |
|-----------|------|
| Steering vectors | ~450MB |
| Checkpoints | ~50-150MB |
| Datasets | ~100MB |
| Results/logs | ~200MB |
| **Total** | ~1-2GB |

---

## üîß Troubleshooting

<details>
<summary><b>Out of Memory (OOM) Errors</b></summary>

1. Reduce batch size in neural bandit config
2. Decrease `--max_train_samples` during training
3. Use fewer warmup samples
4. Ensure previous models are unloaded

</details>

<details>
<summary><b>Slow Training</b></summary>

1. Reduce `--eval-every` to evaluate less frequently
2. Use smaller evaluation set with `--max_eval_samples`
3. Start with LinUCB (fastest) before Neural Bandit
4. Use `--max_train_samples` for quick testing

</details>

<details>
<summary><b>Missing Steering Vectors</b></summary>

```bash
python scripts/create_steering_vectors.py
```

</details>

<details>
<summary><b>W&B Login Issues</b></summary>

```bash
wandb login
# Or disable: python scripts/train_bandit.py --no_wandb
```

</details>

---

## üìú Citation

```bibtex
@article{fair_cb_2026,
  title={Fair-CB: Fairness-Aware Contextual Bandits for Adaptive Multilingual LLM Debiasing},
  author={Your Name},
  journal={Transactions of the Association for Computational Linguistics},
  year={2026},
  note={Under Review}
}
```

---

## üìÑ License

MIT License - see [LICENSE](LICENSE) for details.

---

<div align="center">

**[Documentation](docs/) ¬∑ [Issues](https://github.com/yourusername/Fair-CB/issues) ¬∑ [Contributing](CONTRIBUTING.md)**

</div>
